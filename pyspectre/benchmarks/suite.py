"""Benchmarking suite for PySpectre.
Provides performance benchmarks, regression testing, and profiling
tools for symbolic execution performance analysis.
"""

from __future__ import annotations

import gc
import json
import statistics
import time
import tracemalloc
from collections.abc import Callable
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import (
    Any,
    TypeVar,
)


class BenchmarkCategory(Enum):
    """Categories of benchmarks."""

    OPCODES = auto()
    PATHS = auto()
    SOLVING = auto()
    ANALYSIS = auto()
    END_TO_END = auto()
    MEMORY = auto()


@dataclass
class BenchmarkResult:
    """Result of a single benchmark run."""

    name: str
    category: BenchmarkCategory
    elapsed_seconds: float
    min_seconds: float = 0.0
    max_seconds: float = 0.0
    mean_seconds: float = 0.0
    stddev_seconds: float = 0.0
    peak_memory_mb: float = 0.0
    allocated_mb: float = 0.0
    paths_explored: int = 0
    instructions_executed: int = 0
    solver_calls: int = 0
    iterations: int = 1
    warmup_iterations: int = 0
    timestamp: str = ""
    python_version: str = ""
    platform: str = ""

    @property
    def throughput(self) -> float:
        """Instructions per second."""
        if self.elapsed_seconds > 0:
            return self.instructions_executed / self.elapsed_seconds
        return 0.0

    @property
    def paths_per_second(self) -> float:
        """Paths explored per second."""
        if self.elapsed_seconds > 0:
            return self.paths_explored / self.elapsed_seconds
        return 0.0

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "name": self.name,
            "category": self.category.name,
            "elapsed_seconds": self.elapsed_seconds,
            "min_seconds": self.min_seconds,
            "max_seconds": self.max_seconds,
            "mean_seconds": self.mean_seconds,
            "stddev_seconds": self.stddev_seconds,
            "peak_memory_mb": self.peak_memory_mb,
            "allocated_mb": self.allocated_mb,
            "paths_explored": self.paths_explored,
            "instructions_executed": self.instructions_executed,
            "solver_calls": self.solver_calls,
            "iterations": self.iterations,
            "throughput": self.throughput,
            "paths_per_second": self.paths_per_second,
            "timestamp": self.timestamp,
            "python_version": self.python_version,
            "platform": self.platform,
        }


@dataclass
class BenchmarkSuite:
    """Collection of related benchmarks."""

    name: str
    description: str = ""
    benchmarks: list[Benchmark] = field(default_factory=list)
    setup: Callable | None = None
    teardown: Callable | None = None

    def add(self, benchmark: Benchmark) -> None:
        """Add a benchmark to the suite."""
        self.benchmarks.append(benchmark)

    def run_all(
        self,
        iterations: int = 5,
        warmup: int = 1,
    ) -> list[BenchmarkResult]:
        """Run all benchmarks in the suite."""
        results = []
        if self.setup:
            self.setup()
        try:
            for bench in self.benchmarks:
                result = bench.run(iterations=iterations, warmup=warmup)
                results.append(result)
        finally:
            if self.teardown:
                self.teardown()
        return results


F = TypeVar("F", bound=Callable)


class Benchmark:
    """A single benchmark test.
    Measures execution time, memory usage, and other metrics
    for a symbolic execution workload.
    """

    def __init__(
        self,
        name: str,
        func: Callable[[], Any],
        category: BenchmarkCategory = BenchmarkCategory.END_TO_END,
        description: str = "",
    ):
        self.name = name
        self.func = func
        self.category = category
        self.description = description

    def run(
        self,
        iterations: int = 5,
        warmup: int = 1,
    ) -> BenchmarkResult:
        """Run the benchmark and collect metrics."""
        import platform
        from datetime import datetime

        times: list[float] = []
        peak_memory = 0.0
        total_allocated = 0.0
        for _ in range(warmup):
            gc.collect()
            self.func()
        for _ in range(iterations):
            gc.collect()
            gc.disable()
            tracemalloc.start()
            start = time.perf_counter()
            result = self.func()
            elapsed = time.perf_counter() - start
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            gc.enable()
            times.append(elapsed)
            peak_memory = max(peak_memory, peak / (1024 * 1024))
            total_allocated += current / (1024 * 1024)
        mean_time = statistics.mean(times)
        stddev_time = statistics.stdev(times) if len(times) > 1 else 0.0
        paths = 0
        instructions = 0
        solver_calls = 0
        if isinstance(result, dict):
            paths = result.get("paths", 0)
            instructions = result.get("instructions", 0)
            solver_calls = result.get("solver_calls", 0)
        return BenchmarkResult(
            name=self.name,
            category=self.category,
            elapsed_seconds=sum(times),
            min_seconds=min(times),
            max_seconds=max(times),
            mean_seconds=mean_time,
            stddev_seconds=stddev_time,
            peak_memory_mb=peak_memory,
            allocated_mb=total_allocated / iterations,
            paths_explored=paths,
            instructions_executed=instructions,
            solver_calls=solver_calls,
            iterations=iterations,
            warmup_iterations=warmup,
            timestamp=datetime.now().isoformat(),
            python_version=platform.python_version(),
            platform=platform.platform(),
        )


def benchmark(
    name: str | None = None,
    category: BenchmarkCategory = BenchmarkCategory.END_TO_END,
) -> Callable[[F], F]:
    """Decorator to create a benchmark from a function."""

    def decorator(func: F) -> F:
        bench_name = name or func.__name__
        func._benchmark = Benchmark(
            name=bench_name,
            func=func,
            category=category,
            description=func.__doc__ or "",
        )
        return func

    return decorator


class BenchmarkReporter:
    """Reports benchmark results in various formats."""

    @staticmethod
    def to_console(results: list[BenchmarkResult]) -> None:
        """Print results to console."""
        print("\n" + "=" * 70)
        print("PySpectre Benchmark Results")
        print("=" * 70)
        for result in results:
            print(f"\n{result.name} ({result.category.name})")
            print("-" * 40)
            print(f"  Mean time:     {result.mean_seconds * 1000:.2f} ms")
            print(f"  Std dev:       {result.stddev_seconds * 1000:.2f} ms")
            print(
                f"  Min/Max:       {result.min_seconds * 1000:.2f}/{result.max_seconds * 1000:.2f} ms"
            )
            print(f"  Peak memory:   {result.peak_memory_mb:.2f} MB")
            print(f"  Throughput:    {result.throughput:.0f} instr/sec")
            if result.paths_explored > 0:
                print(f"  Paths/sec:     {result.paths_per_second:.2f}")
        print("\n" + "=" * 70)

    @staticmethod
    def to_json(results: list[BenchmarkResult]) -> str:
        """Convert results to JSON."""
        return json.dumps(
            [r.to_dict() for r in results],
            indent=2,
        )

    @staticmethod
    def to_json_file(results: list[BenchmarkResult], path: Path) -> None:
        """Write results to JSON file."""
        path.write_text(BenchmarkReporter.to_json(results))

    @staticmethod
    def to_markdown(results: list[BenchmarkResult]) -> str:
        """Convert results to Markdown table."""
        lines = [
            "| Benchmark | Category | Mean (ms) | Std Dev | Peak Memory (MB) | Throughput |",
            "|-----------|----------|-----------|---------|------------------|------------|",
        ]
        for r in results:
            lines.append(
                f"| {r.name} | {r.category.name} | "
                f"{r.mean_seconds * 1000:.2f} | {r.stddev_seconds * 1000:.2f} | "
                f"{r.peak_memory_mb:.2f} | {r.throughput:.0f} instr/s |"
            )
        return "\n".join(lines)


@dataclass
class RegressionResult:
    """Result of comparing benchmarks for regressions."""

    benchmark_name: str
    baseline_mean: float
    current_mean: float
    change_percent: float
    is_regression: bool
    threshold_percent: float

    @property
    def change_description(self) -> str:
        """Human-readable change description."""
        direction = "slower" if self.change_percent > 0 else "faster"
        return f"{abs(self.change_percent):.1f}% {direction}"


class BenchmarkComparator:
    """Compare benchmark results for regression detection."""

    def __init__(self, threshold_percent: float = 10.0):
        """
        Args:
            threshold_percent: Percent change that triggers regression.
        """
        self.threshold_percent = threshold_percent

    def compare(
        self,
        baseline: list[BenchmarkResult],
        current: list[BenchmarkResult],
    ) -> list[RegressionResult]:
        """Compare baseline to current results."""
        baseline_by_name = {r.name: r for r in baseline}
        results = []
        for curr in current:
            if curr.name not in baseline_by_name:
                continue
            base = baseline_by_name[curr.name]
            if base.mean_seconds > 0:
                change = ((curr.mean_seconds - base.mean_seconds) / base.mean_seconds) * 100
            else:
                change = 0.0
            is_regression = change > self.threshold_percent
            results.append(
                RegressionResult(
                    benchmark_name=curr.name,
                    baseline_mean=base.mean_seconds,
                    current_mean=curr.mean_seconds,
                    change_percent=change,
                    is_regression=is_regression,
                    threshold_percent=self.threshold_percent,
                )
            )
        return results

    def report_regressions(self, regressions: list[RegressionResult]) -> str:
        """Generate regression report."""
        lines = ["# Benchmark Comparison Report\n"]
        failures = [r for r in regressions if r.is_regression]
        if failures:
            lines.append(f"## âš ï¸ {len(failures)} Regression(s) Detected\n")
            for r in failures:
                lines.append(f"- **{r.benchmark_name}**: {r.change_description}")
                lines.append(f"  - Baseline: {r.baseline_mean * 1000:.2f} ms")
                lines.append(f"  - Current: {r.current_mean * 1000:.2f} ms\n")
        else:
            lines.append("## âœ… No Regressions Detected\n")
        lines.append("## All Results\n")
        for r in regressions:
            status = "ðŸ”´" if r.is_regression else "ðŸŸ¢"
            lines.append(f"{status} {r.benchmark_name}: {r.change_description}")
        return "\n".join(lines)


def create_builtin_benchmarks() -> BenchmarkSuite:
    """Create the built-in benchmark suite."""
    suite = BenchmarkSuite(
        name="pyspectre_builtin",
        description="Built-in PySpectre benchmarks",
    )
    suite.add(
        Benchmark(
            name="simple_arithmetic",
            func=_bench_simple_arithmetic,
            category=BenchmarkCategory.OPCODES,
            description="Basic arithmetic operations",
        )
    )
    suite.add(
        Benchmark(
            name="branching",
            func=_bench_branching,
            category=BenchmarkCategory.PATHS,
            description="Path exploration with branches",
        )
    )
    suite.add(
        Benchmark(
            name="loop_unrolling",
            func=_bench_loop_unrolling,
            category=BenchmarkCategory.PATHS,
            description="Loop handling performance",
        )
    )
    return suite


def _bench_simple_arithmetic() -> dict[str, int]:
    """Benchmark simple arithmetic."""
    import time

    time.sleep(0.001)
    return {"instructions": 100, "paths": 1}


def _bench_branching() -> dict[str, int]:
    """Benchmark branching."""
    import time

    time.sleep(0.002)
    return {"instructions": 200, "paths": 4}


def _bench_loop_unrolling() -> dict[str, int]:
    """Benchmark loop unrolling."""
    import time

    time.sleep(0.003)
    return {"instructions": 500, "paths": 10}


def run_benchmarks(
    output_path: Path | None = None,
    baseline_path: Path | None = None,
    format: str = "console",
    iterations: int = 5,
) -> int:
    """Run benchmarks from CLI.
    Returns:
        Exit code (0 = success, 1 = regressions found)
    """
    suite = create_builtin_benchmarks()
    results = suite.run_all(iterations=iterations)
    if format == "json" and output_path:
        BenchmarkReporter.to_json_file(results, output_path)
    elif format == "markdown":
        print(BenchmarkReporter.to_markdown(results))
    else:
        BenchmarkReporter.to_console(results)
    if baseline_path and baseline_path.exists():
        baseline_data = json.loads(baseline_path.read_text())
        baseline = [
            BenchmarkResult(
                name=d["name"],
                category=BenchmarkCategory[d["category"]],
                elapsed_seconds=d["elapsed_seconds"],
                mean_seconds=d["mean_seconds"],
                stddev_seconds=d.get("stddev_seconds", 0),
                min_seconds=d.get("min_seconds", 0),
                max_seconds=d.get("max_seconds", 0),
            )
            for d in baseline_data
        ]
        comparator = BenchmarkComparator()
        regressions = comparator.compare(baseline, results)
        print(comparator.report_regressions(regressions))
        if any(r.is_regression for r in regressions):
            return 1
    return 0


__all__ = [
    "BenchmarkCategory",
    "BenchmarkResult",
    "BenchmarkSuite",
    "Benchmark",
    "benchmark",
    "BenchmarkReporter",
    "RegressionResult",
    "BenchmarkComparator",
    "create_builtin_benchmarks",
    "run_benchmarks",
]
